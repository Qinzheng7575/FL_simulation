## 环境

- python 3.7.12
- torch 1.4.0
- torchvision 0.5.0
- syft 0.2.4

## 文件结构：

FL_simu.py 是真正运行的代码，剩余代码是为了我方便对照留下的

## 代码架构

按照 FL 的运行逻辑，如下几个步骤:

1. 初始化 ue`init_ue()`,BS`BS_model`，
2. 加载、分发数据(`alloc()`)，开始本地训练(`ue.train()`)，
3. 每训练若干 epoch（聚合频率`train_args['aggre_interval']`），就将 model 进行上传操作(`tranport()`)，**传输函数待完善**
4. BS 聚合模型(`aggregate`)，测试(`test()`)，分发
5. 进入下一轮

## 分级上传版本改进 9.8

- 先打印 model 出来看看
- 寻找量化方法，应用在 tensor 上；量化后还要组合在一起
- 量化运用在 model 上，看前后数据量大小
- 融合进训练中

师姐，给你汇报一下现在的进展。经过几次试错，打算对要传输的 model 进行分级上传的操作如下：将 model（tensor 类）转换为 python 列表，再将列表中的每个元素（float 类，）转换为字符串，再将字符串转化为字节流，对此字节流进行截断就可以实现把一个浮点数分成基础部分和增量部分。
如何将一个列表套列表的复杂矩阵的每一个元素进行这样的精准操作（考虑到后面还需要合并，还要适应多种模型），我打算利用递归方法进行操作！希望此思路能够成功。

https://blog.csdn.net/weixin_55749979/article/details/125574931

打印、保存模型：
https://www.cnblogs.com/shuangcao/p/12030206.html
模型在训练前后分别范围都是\[(+-)7.1041e-05,5.5031e-03,0.3178\]

目前发现，python 保存的都是小数点后 4 位（浮点数表示）

### 对矩阵进行操作

https://zhuanlan.zhihu.com/p/429901066

#### 9.11 记录

今天主要实现了对传输模型的数据分割
遇到如下问题：

- 应该操作的是模型的参数，也就是`model.state_dict()`，然后发现这是一个有序字典，而不是简单的 tensor，其中字典的`value`才是里面每个层的具体参数。
- 由于 python 的赋值都是引用，也就是都是指针而不是值传递，因此注意对列表操作的引用与返回。
- 用`isinstance()`进行数据类型判断
- 使用递归的时候，如果只是对其中的元素进行操作而不需要记录保存的时候，不用 return
- pytorch 打印 tensor 的时候，会因为元素过多而出现省略号这极大的影响了判断，应该在导入包时候加上`torch.set_printoptions(threshold=np.inf)`
  取得结果:
- 确实能够极大减少单次的传输大小，17mb 减小到 9mb

待解决的问题

- 含负号问题
- 科学计数法，含 e-1 问题
- 分片转化为可用的 tensor 问题（解决之后就立刻做仿真实验）
- 分片重组问题

## 半精度浮点数

[`half()`函数快速转换](http://www.manongjc.com/detail/19-ymqghnudzzbpxlh.html)
[一些已有的性能测试](https://zhuanlan.zhihu.com/p/38729704)
[自动混合精度](https://zhuanlan.zhihu.com/p/165152789)
理论上：传输基础值，只是改变精确度， 已经有一些论文理论支持

float16 在精度上足够
如何将一个小数 拆分传输， 编码后代价较小， 并且拆分后的基础值可以直接用于学习。

## 和我们的分级上传有什么关系？

[一种快速数组转换字节流的方法](https://www.codenong.com/58813707/)

### 怎么重新组合？

---

## 改进意见

1. 模拟数字量化过程的精度变化，而不需要真的改变传输的字节流。（0.825 就计数为 3，0.8 就计数为 8）
2. 信道：SIRP？ 先进行信道情况模拟，给一个 rand 随机的变化，计算模型传输的时间，决定能否进行传递，基站评定仿真结果
3. 仿真对比：
   1. 不进行量化，会抛弃模型
   2. 固定量化程度
   3. （未来）自适应量化
4. 基站收到基础值之后：等待一会，如果这个时间内有新增，则基础+新增一起训练；如果没新增，则只训练基础值。
5. 更改量化方式：2 比特：0、0.5；4 比特：0、0.25、0.5、0.75，这样对数据进行量化。（什么是小数的量化？）此方法需结合 1。

---

先想想我现在有什么，在什么基础上进行改进：

一声令下，所有的 ue 都进行一轮的训练，每个产出一个 model

这个 model（类型为 Orderdict），我们要对其进行量化，量化程度是一个可变参数，量化后的大小用计数来表示。
量化之后，就能知道要传输模型的大小了。建立一个模拟信道。计算传输时间。
根据传输时间，判定能传/不能传，基站根据接受之后的计时器，判定继续接受还是纳入计算群众。

基站进行模型聚合，进行训练。

### 小数量化

在模型中，小数部分比整数部分更多，因此先从小数的量化开始（最终可以探究出一个适合整个体系的量化方式，但那是后面寻找的）

简单的说，1 比特量化：0，.5，小数点后只有这两种情况
2bit：0 .25 .5 .75
00 01 10 11
4bit：0 .125 .25 .375 .5 .625 .75 .875
000 001 010 011 100 101 110 111
8bit: 0 0.0625

要量化的数：0.825 1bit:0.5(1) 2bit:0.75(11) 4bit:0.75(110)

2bit:0.75 bias:0.075 8bit:0.0625(0001)
所以先传一个 11，再传一个 0001，最终：0.8125
<mark style="background: #FFB8EBA6;">听起来很不错</mark>
但是数据的动态范围很大，10~10^-5，科学计数法后可否解决?
别忘了 float32，是 32 位。

科学计数法后，小数后都是 4 位，可以以此为基础进行量化，并且初步发现对模型大小很大减少（`getsizeof()`返回单位是 bytes）

### 信道仿真如何模拟？

1. 先不用去纠结具体的值，边界的值设为将来可操作变量。
2. 具体的信道变化情况也不用去真正写出函数，采用随机的方式去模拟。

### 操作难点

- [ ] 数据结构的遍历和重组
- [ ] 数值科学计数法与量化
- [ ] 变量长度计数
- [ ] 信道模拟、传输时延模拟
- [ ] 基站接受策略
- [ ] 最终训练
- [ ] 全局计时

--10.7--
<mark style="background: #FFB8EBA6;">找到一个很好的方法！！！numpy 也有`numpy.float32`这个数据类型，再也不用麻烦处理 python 自己 list 中麻烦的浮点数近似了！</mark>

利用 reshape 将其拉直为一行再进行操作！我咋这么聪明嘞！
