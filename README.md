## 环境

- python 3.7.12
- torch 1.4.0
- torchvision 0.5.0
- syft 0.2.4

## 文件结构：

FL_simu.py 是真正运行的代码，剩余代码是为了我方便对照留下的

## 代码架构

按照 FL 的运行逻辑，如下几个步骤:

1. 初始化 ue`init_ue()`,BS`BS_model`，
2. 加载、分发数据(`alloc()`)，开始本地训练(`ue.train()`)，
3. 每训练若干 epoch（聚合频率`train_args['aggre_interval']`），就将 model 进行上传操作(`tranport()`)，**传输函数待完善**
4. BS 聚合模型(`aggregate`)，测试(`test()`)，分发
5. 进入下一轮

## 分级上传版本改进 9.8

- 先打印 model 出来看看
- 寻找量化方法，应用在 tensor 上；量化后还要组合在一起
- 量化运用在 model 上，看前后数据量大小
- 融合进训练中

师姐，给你汇报一下现在的进展。经过几次试错，打算对要传输的 model 进行分级上传的操作如下：将 model（tensor 类）转换为 python 列表，再将列表中的每个元素（float 类，）转换为字符串，再将字符串转化为字节流，对此字节流进行截断就可以实现把一个浮点数分成基础部分和增量部分。
如何将一个列表套列表的复杂矩阵的每一个元素进行这样的精准操作（考虑到后面还需要合并，还要适应多种模型），我打算利用递归方法进行操作！希望此思路能够成功。

https://blog.csdn.net/weixin_55749979/article/details/125574931

打印、保存模型：
https://www.cnblogs.com/shuangcao/p/12030206.html
模型在训练前后分别范围都是\[(+-)7.1041e-05,5.5031e-03,0.3178\]

目前发现，python 保存的都是小数点后 4 位（浮点数表示）

### 对矩阵进行操作

https://zhuanlan.zhihu.com/p/429901066

#### 9.11 记录

今天主要实现了对传输模型的数据分割
遇到如下问题：

- 应该操作的是模型的参数，也就是`model.state_dict()`，然后发现这是一个有序字典，而不是简单的 tensor，其中字典的`value`才是里面每个层的具体参数。
- 由于 python 的赋值都是引用，也就是都是指针而不是值传递，因此注意对列表操作的引用与返回。
- 用`isinstance()`进行数据类型判断
- 使用递归的时候，如果只是对其中的元素进行操作而不需要记录保存的时候，不用 return
- pytorch 打印 tensor 的时候，会因为元素过多而出现省略号这极大的影响了判断，应该在导入包时候加上`torch.set_printoptions(threshold=np.inf)`
  取得结果:
- 确实能够极大减少单次的传输大小，17mb 减小到 9mb

待解决的问题

- 含负号问题
- 科学计数法，含 e-1 问题
- 分片转化为可用的 tensor 问题（解决之后就立刻做仿真实验）
- 分片重组问题

## 半精度浮点数

[`half()`函数快速转换](http://www.manongjc.com/detail/19-ymqghnudzzbpxlh.html)
[一些已有的性能测试](https://zhuanlan.zhihu.com/p/38729704)
[自动混合精度](https://zhuanlan.zhihu.com/p/165152789)
理论上：传输基础值，只是改变精确度， 已经有一些论文理论支持

float16 在精度上足够
如何将一个小数 拆分传输， 编码后代价较小， 并且拆分后的基础值可以直接用于学习。

## 和我们的分级上传有什么关系？

[一种快速数组转换字节流的方法](https://www.codenong.com/58813707/)

### 怎么重新组合？

---

## 改进意见

1. 模拟数字量化过程的精度变化，而不需要真的改变传输的字节流。（0.825 就计数为 3，0.8 就计数为 8）
2. 信道：SIRP？ 先进行信道情况模拟，给一个 rand 随机的变化，计算模型传输的时间，决定能否进行传递，基站评定仿真结果
3. 仿真对比：
   1. 不进行量化，会抛弃模型
   2. 固定量化程度
   3. （未来）自适应量化
4. 基站收到基础值之后：等待一会，如果这个时间内有新增，则基础+新增一起训练；如果没新增，则只训练基础值。
5. 更改量化方式：2 比特：0、0.5；4 比特：0、0.25、0.5、0.75，这样对数据进行量化。（什么是小数的量化？）此方法需结合 1。

---

先想想我现在有什么，在什么基础上进行改进：

一声令下，所有的 ue 都进行一轮的训练，每个产出一个 model

这个 model（类型为 Orderdict），我们要对其进行量化，量化程度是一个可变参数，量化后的大小用计数来表示。
量化之后，就能知道要传输模型的大小了。建立一个模拟信道。计算传输时间。
根据传输时间，判定能传/不能传，基站根据接受之后的计时器，判定继续接受还是纳入计算群众。

基站进行模型聚合，进行训练。

### 小数量化

在模型中，小数部分比整数部分更多，因此先从小数的量化开始（最终可以探究出一个适合整个体系的量化方式，但那是后面寻找的）

简单的说，1 比特量化：0，.5，小数点后只有这两种情况
2bit：0 .25 .5 .75
00 01 10 11
4bit：0 .125 .25 .375 .5 .625 .75 .875
000 001 010 011 100 101 110 111
8bit: 0 0.0625

要量化的数：0.825 1bit:0.5(1) 2bit:0.75(11) 4bit:0.75(110)

2bit:0.75 bias:0.075 8bit:0.0625(0001)
所以先传一个 11，再传一个 0001，最终：0.8125
<mark style="background: #FFB8EBA6;">听起来很不错</mark>
但是数据的动态范围很大，10~10^-5，科学计数法后可否解决?
别忘了 float32，是 32 位。

科学计数法后，小数后都是 4 位，可以以此为基础进行量化，并且初步发现对模型大小很大减少（`getsizeof()`返回单位是 bytes）

### 信道仿真如何模拟？

1. 先不用去纠结具体的值，边界的值设为将来可操作变量。
2. 具体的信道变化情况也不用去真正写出函数，采用随机的方式去模拟。

### 操作难点

- [x] 数据结构的遍历和重组
- [x] 数值科学计数法与量化
- [x] 变量长度计数
- [x] 信道模拟、传输时延模拟
- [x] 基站接受策略
- [ ] 最终训练
- [ ] 全局计时

--10.7--
<mark style="background: #FFB8EBA6;">找到一个很好的方法！！！numpy 也有`numpy.float32`这个数据类型，再也不用麻烦处理 python 自己 list 中麻烦的浮点数近似了！</mark>

利用 reshape 将其拉直为一行再进行操作！我咋这么聪明嘞！

---

10.8 今天我们先来实现小数量化好不好

> 思路：对单个数进行操作，（所谓科学计数法输出，只有在 print 打印 numpy 数组的适合才会出现，当我进入到单个数的时候，就都还是普通小数了）。后面的思路也完全不需要前面的那种，转换成什么字符串了，直接小数作为参数设计函数！
> 至于矩阵拆分的事情，专门另写函数

print(bin(0+1+1+1+1)) 二进制表示
量化方法：不断除，[似乎还是个算法题嘞](https://www.bilibili.com/read/cv18444080)

10.10
仿真中：
并不需要真的实现量化后的编/解码和传输，只需要计算量化后的 bit 数，得出整体模型的大小，用于传输时延的仿真即可。后续模型计算时采用相应量化后的值。

现在量化后的值已经得出了，该做模型大小计算，与后续通信部分的了。
模型大小如何计算？除了基于前面的计数之外，还需要考虑小数的科学计数法问题吗？想想浮点数是怎么做的，不管多少只要在精度范围内都一个长度的。那我们考虑科学计数法，小数部分的科学计数花 3 个 bit，整数部分花 1 个 bit，剩下的就是按照量化程度了。也就是说，一个数字所需的大小（计数）为 4+n bit,n 为比特量化程度。

10.12
<mark style="background: #FFB8EBA6;">大于 1 的数的小数部分的量化也需要补充上</mark> over！
现在需要写的函数是，传入一个矩阵（oderdict 形式），将其原地进行量化，返回值为（模型大小

似乎有些不和谐的声音？

```
 ('fc.0.weight', tensor([[ 0.0094,  0.0063,
0.0031,  ...,  0.0003,  0.0012, -0.0012],
        [-0.0044, -0.0063, -0.0001,  ..., -0.0003, -0.0069,  0.0063],
        [ 0.0019, -0.0063,  0.0025,  ..., -0.0019, -0.0025, -0.0063],
        ...,
        [ 0.0050, -0.0044,  0.0037,  ..., -0.0081,  0.0025, -0.0063],
        [ 0.0063, -0.0012, -0.0050,  ..., -0.0094,  0.0006,  0.0094],
        [-0.0063,  0.0069,  0.0019,  ...,  0.0037, -0.0037,  0.0088]])),
        'conv.2.weight'
        [[-0.0063,  0.0312, -0.0037],
          [-0.0250,  0.0437, -0.0437],
          [-0.0500,  0.0125,  0.0063]]],


        [[[ 0.0125, -0.0008,  0.0063],
          [ 0.0312,  0.0188, -0.0188],
          [ 0.0037,  0.0063,  0.0312]],
```

原来是虚惊一场！

```python
ddd = np.array([-0.08125, -0.008125, 0.03125])
tensor([-0.0813, -0.0081,  0.0312])
print(torch.from_numpy(ddd))
```

<mark style="background: #FFB8EBA6;">还有，GPU 上的 tensor 不能直接转为 numpy，需要先放到 CPU 上。</mark>

```python
device=torch.device("cpu")#torch.device("cuda")
a=torch.tensor([1,2])
a.to(device)#放到CPU
```

---

### 信道模拟、传输时延模拟

10.18 该正式写信道模拟的部分了

> 通信信道的传输速率与两个方面有关，一个是大尺度衰落，一个是小尺度衰落；从上次会上讨论以及我们内部讨论来看，可以采用简化的手段：
> （1）给节点赋予初始速率，可以是在一个速率范围内随机赋值
> （2）每个节点，在下一个时刻，其速率为=上一时刻速率+速率变化值，其中速率变化值可以简化为一个变动范围内的随机值；
> （3）根据速率，就可以计算出时延

但是仿真过程中具体的数值设置，比如初始速率范围、变化范围，这些参数该到哪里去寻找呢？我感觉是不是得参考一些论文有根据的去设置比较好。
开始漫漫的寻找参考文献之路。。。。

10.19
或许我先不着急寻找最后的参数设置，先用变量来代替！这应该是正确的想法，先把流程跑通吧。
接下来开始正式设计传输时延模拟和基站接受策略的代码！
[[改进仿真#改进意见]]

1. 信道速率函数`Channel_rate()`，按照描述，应该是独立于主程序之外的一个函数，利用多线程？
2. 时延计算函数`Trans_delay(ue,)`
3. 基站接受+聚合策略函数`BS_receive()`
4. 训练函数

   10.22
   多线程思想正确！
   ![](https://gitee.com/qinzheng7575/picgo/raw/master/202203171014741.png)
   <mark style="background: #FFB8EBA6;">可以后面改进的时候试着用动态图去展示信道的速率变化</mark>

基于分段调试的原则，应该在主程序中调试，看每个变量的变化都在控制范围内
10.23
遇到了一个非常奇怪的 bug，'NoneType' object has no attribute 'copy'，并且无论怎么搞都有问题。
经过一下午的测试，终于发现，是装饰器把函数的返回值吞了！！！这个教训值得写一篇博客了。今天还看完了《献给阿尔吉侬的花束》，非常震撼心灵，难以忘怀。

10.31
有必要先进行参数的定义，这样可用快速推进下一步编程！
UE 的初始速率下限 `initial_rate_low`，上限`initial_rate_high`
UE 的速率变化下限`rate_change_low`，`rate_change_high`
BS 接收门限`recv_threshold`，等待门限`wait_threshold`

变量：
UE 类中，`self.channel_rate初始速率、self.trans_delay传输耗时`

---

11.2 开始要最后的收尾冲刺了！冲冲冲！

> 初始速率 1~100Mbps 正态分布
> 变动范围 0~10Mbps 均匀
> 0~50Mbps 正态
> 建立一个模拟信道。计算传输时间。
> 根据传输时间，判定能传/不能传，基站根据接受之后的计时器，判定继续接受还是纳入计算群中。
> 基站进行模型聚合，进行训练。

设计函数的时候，必须结合前后代码来看：
此时遍历 UE_list，对某个 ue 训练好之后的 model 进行操作，进行 Param_compression，得到返回值 data_size 和压缩后的模型。同时，Channel_rate 函数正在并行运行，定期更改着每个 ue 的 self.channel_rate 变量。在 aggregate_with_base_value 对 ue 的 model 进行批量压缩的时候，已经将时延写入到对象里了。

接下来首先应该计算传输时间，判断能不能传递
**问题：得表现仿真出“等一下的感觉”**

11.3
先完善信道模拟的部分，对于正态分布要求的部分，让[0,100]落在 $3\sigma$ 范围中因此 $\mu=50$ , $\sigma=16.67$ ,对于小于 0 的情况直接取绝对值。现在信道速率的仿真已经完毕了！
函数 BS_receive，输入为 UE_list，输出为将要进行聚合的 ue 的 list，先将无法接收到的排除。注意，python 中等号都是引用，因此 ue 必须重新 load 压缩后的参数值，目前 BS_receive 已经编写完毕！

下一步要做的：

- [x] 拉通看一遍代码有没有什么逻辑问题
- [ ] 核对仿真参数，输出待聚合的 list 看一看是否满足仿真的环境，拒绝率有多少（可用给 ue 一个标志位标志自己是 full 还是 2bit 还是 4bit）
- [ ] 正式仿真

  11.4
  现在该考虑具体的拒绝率这些的了，模型大小
  7199292，7.2Mb？

  11.7
  基础量和增加值模型大小是一样的，通过概率也是一样。
  设置三种场景：

1. 环境比较好，模型通过概率为 80% 门限：1.9444
2. 环境一般，通过率为 50% 1.4
3. 环境差，通过概率 30% 1.1864

（刚被一个 if 忘了设置缺省导致遇到 0 出现了无限循环而 debug 半天！）现在能够搭建出基本的框架了，接下来设置好调试打印的信息，再进行仿真！然后再说正式仿真出图

在 aggre_list 的 ue 有哪些？每个 ue 采用的是基础值还是增加值？在进行仿真之前是不是先仿真一下拒绝率？
正式仿真中需要给出参数：不同环境下的模型正确率，loss 的变化图

```python
#一个很有用的函数：用来画一串数字的频率分布直方图
def draw_distribution_histogram(nums, is_hist=True, is_kde=True, is_rug=False,

                                is_vertical=False, is_norm_hist=False):

    sns.set()   # 切换到sns的默认运行配置

    sns.distplot(nums, bins=20, hist=is_hist, kde=is_kde, rug=is_rug,

                 hist_kws={"color": "steelblue"}, kde_kws={"color": "purple"},

                 vertical=is_vertical, norm_hist=is_norm_hist)

    # 添加x轴和y轴标签

    plt.xlabel("XXX")

    plt.ylabel("YYY")

    # 添加标题

    plt.title("Distribution")

    plt.tight_layout()  # 处理显示不完整的问题

    plt.show()


draw_distribution_histogram(y, True, True)
```

```python
#看信道速率的动态变化
model = Net()

UE_list = []

init_ue(10)

t = Thread(target=Channel_rate, args=(UE_list, train_args,), daemon=True)

t.start()

x = []

y = []

for i in range(10):

    y.append([])

plt.ion()

for i in range(20):

    plt.clf()

    x.append(i)

    for num, ue in enumerate(UE_list):

        y[num].append(ue.channel_rate)

    for i, yn in enumerate(y):

        plt.plot(x, yn, label='ue'+str(i+1))

        plt.legend(loc=2)

    plt.pause(0.5)

plt.ioff()

plt.show()
```
